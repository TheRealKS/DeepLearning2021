{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitc0b53b786d4745dc9f5ad1440802b196",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "163bb8d5987a0753d8a45b5389b798e8a36065bb7aa271f0a3973bd9218288f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "from mxnet import autograd, np, npx\n",
    "from d2l import mxnet as d2l\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "source": [
    "# Exercises week 2\n",
    "## 3.1.6.2\n",
    "\n",
    "We want to optimise the cost function: <br>\n",
    "$J(w) = \\parallel y - Xw \\parallel^2$ <br>\n",
    "Partial derivative with respect to $w$: <br>\n",
    "$\\frac{\\partial J(w)}{\\partial w} = X^T(y - Xw)$ <br>\n",
    "Set this to 0 and solve:\n",
    "\\begin{eqnarray*}\n",
    "    X^T(y-Xw) &=& 0 \\\\\n",
    "    w &=& (X^TX)^{-1}X^Ty\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Since this is a closed formula, it will work better than gradient descent if we can computationally handle the size of matrix $X$. If this is not the case, stochastic gradient descent (because of minibatching) works better."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.2.9.1\n",
    "\n",
    "If the squared loss function is used, initialising the weights to zero would only change the number of epochs needed to reach convergence. Since the squared loss function is convex, it only has one critical point, which means that the initial values of the weights does not determine whether that critical point will be reached, only when."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.2.9.6\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1, loss 0.000051\n",
      "epoch 2, loss 0.000051\n",
      "epoch 3, loss 0.000050\n",
      "error in estimating w: [-0.0007329 -0.0006671]\n",
      "error in estimating b: [0.00031614]\n"
     ]
    }
   ],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = np.random.normal(0, 1, (num_examples, len(w)))\n",
    "    y = np.dot(X, w) + b\n",
    "    y += np.random.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = np.array([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = np.array(indices[i:min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "w = np.random.normal(0, 0.01, (2, 1))\n",
    "b = np.zeros(1)\n",
    "w.attach_grad()\n",
    "b.attach_grad()\n",
    "\n",
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"The linear regression model.\"\"\"\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 / 2\n",
    "\n",
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size\n",
    "\n",
    "lr = 0.1\n",
    "num_epochs = 3\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with autograd.record():\n",
    "            l = loss(net(X, w, b), y)  # Minibatch loss in `X` and `y`\n",
    "        # Because `l` has a shape (`batch_size`, 1) and is not a scalar\n",
    "        # variable, the elements in `l` are added together to obtain a new\n",
    "        # variable, on which gradients with respect to [`w`, `b`] are computed\n",
    "        l.backward()\n",
    "        sgd([w, b], lr, batch_size)  # Update parameters using their gradient\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n",
    "\n",
    "print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'error in estimating b: {true_b - b}')"
   ]
  },
  {
   "source": [
    "For learning rate 0.03: <br>\n",
    "epoch 1, loss 0.037290\n",
    "epoch 2, loss 0.000132\n",
    "epoch 3, loss 0.000048\n",
    "error in estimating w: [ 0.00096571 -0.00061107]\n",
    "error in estimating b: [0.00026894]\n",
    "\n",
    "For learning rate 0.01: <br>\n",
    "epoch 1, loss 2.212858\n",
    "epoch 2, loss 0.294010\n",
    "epoch 3, loss 0.039279\n",
    "error in estimating w: [ 0.11729062 -0.17064309]\n",
    "error in estimating b: [0.18917513]\n",
    "\n",
    "For learning rate 0.05: <br>\n",
    "epoch 1, loss 0.000650\n",
    "epoch 2, loss 0.000047\n",
    "epoch 3, loss 0.000048\n",
    "error in estimating w: [6.9332123e-04 1.5974045e-05]\n",
    "error in estimating b: [-0.00017262]\n",
    "\n",
    "For learning rate 0.1: <br>\n",
    "epoch 1, loss 0.000051\n",
    "epoch 2, loss 0.000051\n",
    "epoch 3, loss 0.000050\n",
    "error in estimating w: [-0.0007329 -0.0006671]\n",
    "error in estimating b: [0.00031614]\n",
    "\n",
    "It seems that a higher learning rate will increase the rate of decrease for the loss function significantly, even with small changes, as can be seen in the change from 0.01 to 0.03. If the learning rate is very high (here: 0.1), the change in loss is minimal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.6.9.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def softmax(X):\n",
    "    X_exp = np.exp(X)\n",
    "    partition = X_exp.sum(1, keepdims=True)\n",
    "    return X_exp / partition\n",
    "\n",
    "X = np.random.normal(0, 1, (2, 5))\n",
    "X_prob = softmax(X)\n",
    "X_prob, X_prob.sum (1)\n",
    "\n",
    "print(sys.getsizeof(np.exp(50)))"
   ]
  },
  {
   "source": [
    "We now see that the size of exp(50) in memory is 32 bytes. If the arrays become large, this means that we will get very large result arrays, which take up a lot of space in memory. Since large arrays and lots of iterations are necessary to perform regression, this might become a problem.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.6.9.4\n",
    "\n",
    "In some cases it is probably better to return a list of labels, ordered by likelihood. So for, as stated in the exercise, if we have a medical diagnosis, we could return a list of possible diagnosis, each labeled by how likely they are. This will allow the system to help a doctor make an informed diagnosis, by combining the output of the system and their own expertise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}