{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitc0b53b786d4745dc9f5ad1440802b196",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "163bb8d5987a0753d8a45b5389b798e8a36065bb7aa271f0a3973bd9218288f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Exercises week 8\n",
    "\n",
    "## 1: **Deconvolution (fractionally strided convolution) layer is used to increase the spatial size of feature maps of a convnet. Briefly explain how deconvolution works.**\n",
    "\n",
    "(Strided) Convolution will downsample the input. Deconvolution is the opposite of this. We can see it easily as simply adding interleaving some padding (zeroes), then performing the convolution as normal. The padded zeroes will be lost by the downsampling, but the original input is preserved. \n",
    "\n",
    "## 2: **Deconvolution layer can lead to checkerboard artefact. Briefly explain what is meant by checkerboard artefact and how it can be overcome.**\n",
    "In some cases, the padding is preserved after the convolution, mainly when the amount of padding after each 'real' input is not divisible by the stride size. Eventually, this causes artifacting, which is what is meant by the checkerboard artefact. It can be overcome by not using zeroes for the interleaved padding. Instead, we can simply upscale the input, using something like bilinear or bicubic scaling. Then, when the input is downsampled by the convolution, we do not preserve artifacts, but plausible data.\n",
    "\n",
    "## 3: **How do residual connections counter the potential problems of having many layers in a network? Briefly explain.**\n",
    "Residual connections make it easier to learn the idenity function, which essentially just passes down the input to the deeper layers. If we have too many layers, we can have some of these layers learn the identity function, which means that they essentially do nothing (the network might as well not have had these layers). So, it is a way to 'bypass' layers to create a smaller network out of a bigger one.\n",
    "\n",
    "## 4: **Briefly compare pixel loss and feature loss. What are their shortcomings and advantages?**\n",
    "Pixel loss works by summing the absolute errors between output and target pixels (computed using Euclidean distance). A drawback is that because of its absoluteness, it does not accurately capture meaningful information in the context of training a network, but in some cases this absoluteness might actually be desireable.\n",
    "\n",
    "Feature loss works by taking the mean of the squared errors between output and target pixels. This better captures high level differences in content and style, for example, but is less precise than absolute pixel loss."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}