{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitc0b53b786d4745dc9f5ad1440802b196",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "163bb8d5987a0753d8a45b5389b798e8a36065bb7aa271f0a3973bd9218288f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Exercises week 1\n",
    "## 2.1.8.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.  1.  2.  3.]\n [ 4.  5.  6.  7.]\n [ 8.  9. 10. 11.]]\n[[2. 1. 4. 3.]\n [1. 2. 3. 4.]\n [4. 3. 2. 1.]]\n[[False  True False  True]\n [False False False False]\n [False False False False]]\n[[ True False  True False]\n [False False False False]\n [False False False False]]\n[[False False False False]\n [ True  True  True  True]\n [ True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "X = np.arange(12).reshape(3, 4)\n",
    "Y = np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "np.concatenate([X, Y], axis=0), np.concatenate([X, Y], axis=1)\n",
    "print(X)\n",
    "print(Y)\n",
    "print(X == Y)\n",
    "print(X < Y)\n",
    "print(X > Y)"
   ]
  },
  {
   "source": [
    "If we use X < Y or X > Y, instead of X == Y, the resulting tensor becomes different, because each element in the resulting tensor is based on the comparison between the two source tensors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.1.8.2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[0.],\n",
       "        [1.],\n",
       "        [2.]]),\n",
       " array([[0., 1.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "a = np.arange(3).reshape(3, 1)\n",
    "b = np.arange(2).reshape(1, 2)\n",
    "a = np.arange(3).reshape(3, 1)\n",
    "b = np.arange(3).reshape(1, 3)\n",
    "#a = np.random.normal(0, 1, size=(3, 3))\n",
    "#b = np.random.normal(0, 1, size=(3, 3))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 2.],\n",
       "       [2., 3.]])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "source": [
    "If we replace these tensors by 3 dimensional tensors, we get a three dimensional tensor back, this follows from the properties of matrices."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.2.5 (1 and 2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    NumRooms Alley   Price\n0        NaN  Pave  170922\n1        NaN   NaN  157307\n2        NaN  Pave  116050\n3        NaN   NaN  126622\n4        NaN   NaN  127878\n..       ...   ...     ...\n95       NaN   NaN  131340\n96       3.0  Pave  170906\n97       1.0  Pave  162121\n98       NaN  Pave  162252\n99       NaN  Pave  188424\n\n[100 rows x 3 columns]\n     Price  Alley_Pave  Alley_nan\n0   170922           1          0\n1   157307           0          1\n2   116050           1          0\n3   126622           0          1\n4   127878           0          1\n..     ...         ...        ...\n95  131340           0          1\n96  170906           1          0\n97  162121           1          0\n98  162252           1          0\n99  188424           1          0\n\n[100 rows x 3 columns]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.70922e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.57307e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.16050e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.26622e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.27878e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.55627e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.66141e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.87854e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.83804e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.82898e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.42542e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.93442e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.00018e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.89532e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.32358e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.04663e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.16299e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.86378e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.55318e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.22030e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.12839e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.45547e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.33506e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.47718e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.71128e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.19198e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.07540e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.27072e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.85856e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.93074e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.93953e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.01222e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.34187e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.35669e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.45275e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.32130e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.59010e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.41374e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.74760e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.50934e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.32227e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.75458e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.58240e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.70583e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.41966e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.71003e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.91109e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.20000e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.79531e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.22726e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.60756e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.02073e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.99787e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.12044e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.95202e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.40458e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.43690e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.14362e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.53981e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.90612e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.62794e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.05509e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.29600e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.35277e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.21972e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.95945e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.06974e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.53023e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.89451e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.89625e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.63045e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.96044e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.13863e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.07073e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.98437e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.02570e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.02664e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.90507e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.90163e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.78849e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.31002e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.99719e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.83292e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.18234e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.64286e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.19648e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.24112e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.74986e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.82930e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.08649e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.88449e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.47614e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.20736e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.91009e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.39471e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.31340e+05, 0.00000e+00, 1.00000e+00],\n",
       "       [1.70906e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.62121e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.62252e+05, 1.00000e+00, 0.00000e+00],\n",
       "       [1.88424e+05, 1.00000e+00, 0.00000e+00]])"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "os.makedirs(os.path.join('.', 'data'), exist_ok=True)\n",
    "data_file = os.path.join('.', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')  # Column names\n",
    "    for i in range(0,100):\n",
    "        entry = ''\n",
    "        flip = random.randint(0, 1)\n",
    "        if (flip == 0):\n",
    "            entry += 'NA,'\n",
    "        else:\n",
    "            entry += str(random.randint(0,7)) + ','\n",
    "        flip = random.randint(0,1)\n",
    "        if (flip == 0):\n",
    "            entry += 'Pave,'\n",
    "        else:\n",
    "            entry += 'NA,'\n",
    "        entry += str(random.randint(100000,200000)) + '\\n'\n",
    "        f.write(entry)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)\n",
    "\n",
    "cols = [data.iloc[:, 0].isna().sum(), data.iloc[:, 1].isna().sum()]\n",
    "data = data.drop(data.columns[cols.index(max(cols))], axis=1)\n",
    "if (data.columns[0] == 'NumRooms'):\n",
    "    data = data.fillna(0)\n",
    "else:\n",
    "    data = pd.get_dummies(data, dummy_na=True)\n",
    "\n",
    "print(data)\n",
    "X = np.array(data)\n",
    "X"
   ]
  },
  {
   "source": [
    "## 2.3.13.1\n",
    "Suppose we have a matrix $A$. Then for $A^T$, $a^T_{ij} = a_{ji}$. If we then take $(A^T)^T$, we have that $(a^T)^T_{ij} = a^T_{ji} = a_{ij}$. Because of this it follows that $(A^T)^T = A$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.3.13.2\n",
    "\n",
    "Suppose we have two $m$ x $n$ matrices $A$ and $B$. <br>\n",
    "We have, by definition that for $A^T, a^T_{mn} = a_{nm}$, and analogously for $B^T$. <br>\n",
    "We then have $A^T + B^T = \\forall_{mn} a_{nm} + b_{nm} = \\forall_{mn} a^T_{mn} + b^T_{mn} = (A + B)^T$. <br>\n",
    "This shows that indeed $A^T + B^T = (A + B)^T$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.3.13.3\n",
    "\n",
    "Yes. <br>\n",
    "We have that:\n",
    "* $(A^T)^T = A$ \n",
    "* $(A + B)^T = A^T + B^T$\n",
    "\n",
    "\n",
    "If we have $X = A^T + A$, we have $X^T = (A^T + A)^T = (A^T + (A^T)^T)^T = (A^T)^T + A^T = A^T + A = X$. \n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.4.6.1\n",
    "\n",
    "![Image](./images/tangent_w1_calc.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.4.6.2\n",
    "\n",
    "In this function, there are two variables, and the gradient is a vector containing the all the partial derivatives with respect to all variables in the function. So, the gradient of $f(x) = [6x_1, 5e^{x_2}]$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.4.6.3\n",
    "\n",
    "Since $\\parallel x \\parallel_2 = \\sqrt{x^2_1 + \\dots + x^2_n}$, the gradient of $f(x) = \\langle \\frac{x_1}{\\sqrt{x^2_1 + \\dots + x^2_n}},\\dots,\\frac{x_n}{\\sqrt{x^2_1 + \\dots + x^2_n}} \\rangle$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.4.6.4\n",
    "\n",
    "$\\frac{\\partial u}{\\partial a} = \\frac{\\partial u}{\\partial x}\\frac{\\partial x}{\\partial a} + \\frac{\\partial u}{\\partial y}\\frac{\\partial y}{\\partial a} + \\frac{\\partial u}{\\partial z}\\frac{\\partial z}{\\partial a}$, and analogously for $b$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.5.6.1\n",
    "\n",
    "The computational graph will increase in size exponentially. For each partial derivative in the gradient (which contains all first derivatives), we get another chain of partial derivatives."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.5.6.2\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "\n",
    "x = np.arange(4.0)\n",
    "x.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    y = 2 * np.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.,  4.,  8., 12.])"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "y.backward(retain_graph=True)\n",
    "x.grad"
   ]
  },
  {
   "source": [
    "We see that if we run the function again, we get an error, because the computational graph is not retained in memory. We can set the parameter `retain_graph` to `True` to prevent this error, but then the result is the same every time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}